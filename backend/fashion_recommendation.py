# -*- coding: utf-8 -*-
"""Fashion_Recommendation_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QGgvGZVnPq4XOZeLeZsC4Rg3bKOjgst8
"""

from google.colab import drive
drive.mount('/content/drive')

import os
folder_path = '/content/drive/My Drive/archive.zip'

os.listdir('/content/drive')

import numpy as np
import pickle as pkl
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.layers import GlobalMaxPool2D
from sklearn.neighbors import NearestNeighbors
import os
from numpy.linalg import norm

import zipfile
import os

folder_path = '/content/drive/My Drive/archive.zip'

# Extract 'images' folder from archive.zip
with zipfile.ZipFile(folder_path, 'r') as zip_ref:
  zip_ref.extractall('/content/extracted_images')

# Path to the folder containing the images
image_folder_path = '/content/extracted_images/images'  # Update path if 'images' is in a subfolder

# Get the full path for each image file
filenames = [os.path.join(image_folder_path, file) for file in os.listdir(image_folder_path) if os.path.isfile(os.path.join(image_folder_path, file))]

# Print the full image paths
print(filenames)

len(filenames)

#Importing ResNet50 Model and Configuration

model=ResNet50(weights='imagenet',include_top=False, input_shape=(224,224,3))
model.trainable=False
# Create the sequential model with an input layer
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),  # Define input shape here
    model,
    GlobalMaxPool2D()
])
model.summary()

#Extracting features from Image

# Path to the image
img_path = '/content/extracted_images/images/2339.jpg'

# Verify if the file exists
if os.path.exists(img_path):
    img = image.load_img(img_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_expand_dim = np.expand_dims(img_array, axis=0)
    img_preprocess = preprocess_input(img_expand_dim)
    result = model.predict(img_preprocess).flatten()
    norm_result = result / norm(result)
    print(norm_result)
else:
    print(f"File {img_path} does not exist. Please check the path and try again.")

result

def extract_features_from_images(image_path, model):
  img=image.load_img(image_path, target_size=(224,224))
  img_array=image.img_to_array(img)
  img_expand_dim=np.expand_dims(img_array,axis=0)
  img_preprocess=preprocess_input(img_expand_dim)
  result=model.predict(img_preprocess).flatten()
  norm_result=result/norm(result)
  return norm_result

extract_features_from_images(filenames[0],model)

image_features=[]
for file in filenames[0:5]:
  image_features.append(extract_features_from_images(file,model))
image_features

from concurrent.futures import ThreadPoolExecutor
import numpy as np
import os

# Adjusted function to extract features from a batch in parallel
def process_batch(batch_files, model):
    batch_features = [extract_features_from_images(file, model) for file in batch_files]
    return np.array(batch_features)

# Define the batch size and number of workers
batch_size = 5
num_workers = 4  # Adjust based on available CPU/GPU cores in Colab

# Main extraction loop with parallel processing
for start in range(0, len(filenames), batch_size):
    end = min(start + batch_size, len(filenames))
    batch_files = filenames[start:end]

    # Parallel feature extraction
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        batch_features = list(executor.map(lambda file: extract_features_from_images(file, model), batch_files))

    # Convert to numpy array and save batch
    batch_features = np.array(batch_features)
    batch_index = start // batch_size
    np.save(f'/content/drive/MyDrive/feature_batches/batch_{batch_index}.npy', batch_features)
    print(f"Processed batch {batch_index} ({start} to {end})")

print("Feature extraction completed.")

import os
import numpy as np

# Define paths and parameters
output_dir = '/content/drive/MyDrive/feature_batches'  # Path where batches are saved
total_images = len(filenames)  # Total number of images processed
batch_size = 500  # Batch size used during extraction
expected_batches = (total_images + batch_size - 1) // batch_size  # Calculate expected number of batches

# List saved batch files in the output directory
saved_batches = sorted([file for file in os.listdir(output_dir) if file.endswith('.npy')])

# Check if the number of saved files matches the expected number of batches
if len(saved_batches) == expected_batches:
    print(f"All batches saved successfully! ({len(saved_batches)}/{expected_batches} batches found)")
else:
    print(f"Warning: Some batches are missing! ({len(saved_batches)}/{expected_batches} batches found)")

# Optionally, verify contents by loading and checking a few batch files
for i in range(min(3, len(saved_batches))):  # Load first 3 batches to check content
    batch_path = os.path.join(output_dir, saved_batches[i])
    batch_data = np.load(batch_path)
    print(f"Batch {i} shape: {batch_data.shape}")

# prompt: how to make all the saved batches in google drive into a single folder

import os
import shutil

# Define the source directory where your batches are stored
source_dir = '/content/drive/MyDrive/feature_batches'

# Define the destination directory where you want to create the single folder
destination_dir = '/content/drive/MyDrive/all_feature_batches'

# Create the destination directory if it doesn't exist
os.makedirs(destination_dir, exist_ok=True)

# Move all files from the source directory to the destination directory
for filename in os.listdir(source_dir):
    source_file = os.path.join(source_dir, filename)
    destination_file = os.path.join(destination_dir, filename)
    shutil.move(source_file, destination_file)

print(f"All files moved from '{source_dir}' to '{destination_dir}'.")

# Optionally, you can delete the original source directory if needed
# shutil.rmtree(source_dir)

# prompt: how to display the size of all_feature_batches

# Get the list of files in the all_feature_batches directory
all_feature_batches_files = os.listdir('/content/drive/MyDrive/all_feature_batches')

# Initialize the total size
total_size = 0

# Iterate through the files and calculate the size of each one
for file in all_feature_batches_files:
    file_path = os.path.join('/content/drive/MyDrive/all_feature_batches', file)
    file_size = os.path.getsize(file_path)
    total_size += file_size

# Print the total size in MB
print(f"Total size of all_feature_batches: {total_size / (1024 * 1024):.2f} MB")

# prompt: how to display the number of extracted features in that folder

# Define the directory where the feature batches are stored
feature_batches_dir = '/content/drive/MyDrive/all_feature_batches'

# Get a list of all the .npy files (feature batches) in the directory
feature_batch_files = [f for f in os.listdir(feature_batches_dir) if f.endswith('.npy')]

# Initialize a counter for the total number of extracted features
total_extracted_features = 0

# Iterate through each feature batch file
for file in feature_batch_files:
  # Load the feature batch from the file
  file_path = os.path.join(feature_batches_dir, file)
  batch_data = np.load(file_path)

  # Get the number of features in the current batch
  num_features_in_batch = batch_data.shape[0]

  # Add the number of features in this batch to the total count
  total_extracted_features += num_features_in_batch

# Print the total number of extracted features
print(f"Total number of extracted features: {total_extracted_features}")

#Finding similar images with nearest neighbours

neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='euclidean')

import numpy as np
import os

# Define the path to the directory containing all feature batches
batches_dir = '/content/drive/MyDrive/all_feature_batches'

# Initialize an empty list to store features from each batch
all_image_features = []

# Load each batch file and append the features to all_image_features
for batch_file in sorted(os.listdir(batches_dir)):
    if batch_file.endswith('.npy'):  # Check that the file is a .npy file
        batch_path = os.path.join(batches_dir, batch_file)
        batch_data = np.load(batch_path)
        all_image_features.append(batch_data)

# Concatenate all batch features into a single numpy array
image_features = np.vstack(all_image_features)

# Check the shape to ensure itâ€™s loaded correctly
print("Combined image_features shape:", image_features.shape)

import numpy as np
import os

# Define the path to the directory containing all feature batches
batches_dir = '/content/drive/MyDrive/all_feature_batches'

# Initialize an empty list to store features from each batch
all_image_features = []

# Load each batch file and append the features to all_image_features
for batch_file in sorted(os.listdir(batches_dir)):
    batch_path = os.path.join(batches_dir, batch_file)
    batch_data = np.load(batch_path)
    all_image_features.append(batch_data)

# Concatenate all batch features into a single numpy array
image_features = np.vstack(all_image_features)

# Check if image_features is loaded correctly
if image_features is None or len(image_features) == 0:
    print("Error: image_features is empty. Please ensure that features have been correctly loaded.")
else:
    print("image_features loaded successfully. Shape:", image_features.shape)

# Initialize and fit the NearestNeighbors model
neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='euclidean')
neighbors.fit(image_features)

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import GlobalMaxPooling2D
import tensorflow as tf

# Recreate the ResNet50 model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
model = tf.keras.Sequential([
    base_model,
    GlobalMaxPooling2D()
])
model.trainable = False  # Freeze the model weights

import os
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input
import numpy as np
from numpy.linalg import norm

def extract_features_from_images(image_path, model):
    """
    Extracts features from a given image using a pre-trained model.

    Args:
        image_path (str): The path to the image file.
        model (tensorflow.keras.Model): The pre-trained model.

    Returns:
        numpy.ndarray: The extracted features as a flattened array.
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Error: File {image_path} does not exist.")

    # Load and preprocess the image
    img = image.load_img(image_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_expand_dim = np.expand_dims(img_array, axis=0)
    img_preprocess = preprocess_input(img_expand_dim)

    # Extract features using the model's predict function
    result = model.predict(img_preprocess).flatten()

    # Normalize the result
    norm_result = result / norm(result)
    return norm_result

# Path to the image file
image_path = '/content/extracted_images/images/2339.jpg'  # Update the path as needed

# Call the function
try:
    input_image = extract_features_from_images(image_path, model)
    print("Extracted features for input image:", input_image)
except FileNotFoundError as e:
    print(e)

distance,indices=neighbors.kneighbors([input_image])

indices[0]

from IPython.display import Image

# Check the raw features before normalization
raw_result = result
norm_result = raw_result / norm(raw_result)
print("Raw features:", raw_result)
print("Normalized features:", norm_result)

from sklearn.neighbors import NearestNeighbors

# Initialize NearestNeighbors with cosine metric
neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='cosine')

# Fit the model
neighbors.fit(image_features)

# Find the nearest neighbors for the input image
distance, indices = neighbors.kneighbors([input_image])

print(f"Feature vector shape before normalization: {result.shape}")
print(f"Feature vector shape after normalization: {norm_result.shape}")

import os
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input
import numpy as np

# Define the image path
image_path = '/content/extracted_images/images/2339.jpg'

# Check if the file exists
if not os.path.exists(image_path):
    print(f"Error: File {image_path} not found.")
else:
    # Load and preprocess the image
    img = image.load_img(image_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_expand_dim = np.expand_dims(img_array, axis=0)
    img_preprocess = preprocess_input(img_expand_dim)
    print("Image preprocessing completed.")

print("Shape of all image features:", image_features.shape)

import os
from sklearn.neighbors import NearestNeighbors
import numpy as np

# Verify file existence
test_image_path = '/content/2339.jpg'
if not os.path.exists(test_image_path):
    print(f"Error: File {test_image_path} not found. Using a default image.")
    test_image_path = subset_filenames[0]  # Use the first image in the subset

# Test similarity for one image
try:
    input_image = extract_features_from_images(test_image_path, model)
    distance, indices = neighbors.kneighbors([input_image])
    print("Recommended indices:", indices)
except Exception as e:
    print(f"An error occurred: {e}")

distance,indices=neighbors.kneighbors([input_image])

indices[0]

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Apply PCA to reduce feature dimensions to 2D
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(image_features)

# Plot the features in 2D space
plt.scatter(reduced_features[:, 0], reduced_features[:, 1], s=5)
plt.title("2D PCA of Image Features")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Fine-tune the last few layers
for layer in base_model.layers[:-10]:  # Freeze all but the last 10 layers
    layer.trainable = False

# Recreate the full model
model = tf.keras.Sequential([
    base_model,
    GlobalMaxPooling2D()
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Try Manhattan Distance instead of Euclidean
neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='manhattan')

print("Shape of features in batch:", batch_data.shape)

print("Distances to nearest neighbors:", distance)

import os
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input
import numpy as np

def extract_features_from_images(image_path, model):
    """
    Extracts features from a given image using a pre-trained model.

    Args:
        image_path (str): The path to the image file.
        model (tensorflow.keras.Model): The pre-trained model.

    Returns:
        numpy.ndarray: The extracted features as a flattened array.
    """
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Error: File {image_path} does not exist.")

    img = image.load_img(image_path, target_size=(224, 224))
    img_array = image.img_to_array(img)
    img_expand_dim = np.expand_dims(img_array, axis=0)
    img_preprocess = preprocess_input(img_expand_dim)
    result = model.predict(img_preprocess).flatten()
    norm_result = result / np.linalg.norm(result)
    return norm_result

# Debug and fallback logic
image_path = '2339.jpg'

# Define a small subset for debugging
subset_filenames = filenames[:10]  # Ensure this list is populated correctly

# Choose the default image if the specified path doesn't exist
if not os.path.exists(image_path):
    print(f"Warning: {image_path} not found. Using default image from subset: {subset_filenames[0]}")
    image_path = subset_filenames[0]  # Default to the first image in the subset

# Extract features from the chosen image
input_image = extract_features_from_images(image_path, model)

# Display the extracted features or use them for further processing
print("Extracted features for the input image:", input_image)

from sklearn.neighbors import NearestNeighbors
import os

# Ensure subset_filenames is correctly defined and populated
subset_filenames = filenames[:10]  # Use only the first 10 images for debugging

# Fit the Nearest Neighbors model to the batch data
batch_data = [extract_features_from_images(file, model) for file in subset_filenames]
batch_data = np.array(batch_data)

neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='cosine')
neighbors.fit(batch_data)

# Check if the specified image exists, otherwise use the first image from the subset
image_path = '2339.jpg'
if not os.path.exists(image_path):
    print(f"Warning: {image_path} not found. Using default image from subset: {subset_filenames[0]}")
    image_path = subset_filenames[0]

# Extract features and find neighbors for the selected image
input_image = extract_features_from_images(image_path, model)
distance, indices = neighbors.kneighbors([input_image])

# Display the results
print("Recommended indices:", indices)

indices[0]

from IPython.display import Image

import zipfile
import os

# Path to the zip file and extract location
zip_path = '/content/drive/MyDrive/archive.zip'  # Update with your actual zip file path
extract_path = '/content/extracted_images'

# Extract the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extraction complete. Files are in:", os.listdir(extract_path))

import os

# List all files and directories in the extracted folder
for root, dirs, files in os.walk(extract_path):
    print(f"Directory: {root}")
    for file in files:
        print(f"File: {file}")

image_name = '21217.jpg'
found_path = None

for root, dirs, files in os.walk(extract_path):
    if image_name in files:
        found_path = os.path.join(root, image_name)
        break

if found_path:
    print("Image found at:", found_path)
else:
    print("Image not found!")

from IPython.display import Image, display

# Display the image
image_path = "/content/extracted_images/images/21217.jpg"  # Enclose the path in quotes
display(Image(filename=image_path))

Image(filenames[indices[0][0]])

Image(filenames[indices[0][1]])

Image(filenames[indices[0][2]])

Image(filenames[indices[0][3]])

Image(filenames[indices[0][4]])

